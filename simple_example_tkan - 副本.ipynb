{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb186818-bd1d-46ed-a018-27efa013b206",
   "metadata": {},
   "source": [
    "# TKAN example and comparison with benchmarks\n",
    "\n",
    "All test have been run on a RTX 4070 with an Core™ i7-6700K on vast.ai using this [jax docker image](https://hub.docker.com/r/bitnami/jax/)\n",
    "\n",
    "tkan version: 0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc3f1ac2-1785-4e08-89a5-0bc5e31ce2b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\anaconda\\envs\\dl\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\dl\\lib\\site-packages (1.24.4)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\envs\\dl\\lib\\site-packages (3.9.2)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-17.0.0-cp39-cp39-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\envs\\dl\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: tkan in d:\\anaconda\\envs\\dl\\lib\\site-packages (0.4.1)\n",
      "Collecting jax[cuda12]\n",
      "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from matplotlib) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: keras<4.0,>=3.0.0 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from tkan) (3.0.0)\n",
      "Requirement already satisfied: keras_efficient_kan<0.2.0,>=0.1.4 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from tkan) (0.1.4)\n",
      "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax[cuda12])\n",
      "  Downloading jaxlib-0.4.30-cp39-cp39-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from jax[cuda12]) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in d:\\anaconda\\envs\\dl\\lib\\site-packages (from jax[cuda12]) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from jax[cuda12]) (7.0.0)\n",
      "INFO: pip is looking at multiple versions of jax[cuda12] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax[cuda12]\n",
      "  Downloading jax-0.4.29-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting ml-dtypes>=0.4.0 (from jax[cuda12])\n",
      "  Downloading ml_dtypes-0.5.0-cp39-cp39-win_amd64.whl.metadata (22 kB)\n",
      "Collecting jaxlib==0.4.29 (from jax[cuda12])\n",
      "  Downloading jaxlib-0.4.29-cp39-cp39-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting jax[cuda12]\n",
      "  Downloading jax-0.4.28-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting jaxlib==0.4.28 (from jax[cuda12])\n",
      "  Downloading jaxlib-0.4.28-cp39-cp39-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting jax[cuda12]\n",
      "  Downloading jax-0.4.27-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting jaxlib==0.4.27 (from jax[cuda12])\n",
      "  Downloading jaxlib-0.4.27-cp39-cp39-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting jax[cuda12]\n",
      "  Downloading jax-0.4.26-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting jaxlib==0.4.26 (from jax[cuda12])\n",
      "  Downloading jaxlib-0.4.26-cp39-cp39-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting jax[cuda12]\n",
      "  Downloading jax-0.4.25-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting jaxlib==0.4.25 (from jax[cuda12])\n",
      "  Downloading jaxlib-0.4.25-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting jax[cuda12]\n",
      "  Downloading jax-0.4.24-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading jax-0.4.23-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting jaxlib==0.4.23 (from jax[cuda12])\n",
      "  Downloading jaxlib-0.4.23-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "INFO: pip is still looking at multiple versions of jax[cuda12] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax[cuda12]\n",
      "  Downloading jax-0.4.22-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting jaxlib==0.4.22 (from jax[cuda12])\n",
      "  Downloading jaxlib-0.4.22-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting jax[cuda12]\n",
      "  Downloading jax-0.4.21-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from importlib-metadata>=4.6->jax[cuda12]) (3.20.2)\n",
      "Requirement already satisfied: absl-py in d:\\anaconda\\envs\\dl\\lib\\site-packages (from keras<4.0,>=3.0.0->tkan) (2.0.0)\n",
      "Requirement already satisfied: rich in d:\\anaconda\\envs\\dl\\lib\\site-packages (from keras<4.0,>=3.0.0->tkan) (13.7.1)\n",
      "Requirement already satisfied: namex in d:\\anaconda\\envs\\dl\\lib\\site-packages (from keras<4.0,>=3.0.0->tkan) (0.0.7)\n",
      "Requirement already satisfied: h5py in d:\\anaconda\\envs\\dl\\lib\\site-packages (from keras<4.0,>=3.0.0->tkan) (3.10.0)\n",
      "Requirement already satisfied: dm-tree in d:\\anaconda\\envs\\dl\\lib\\site-packages (from keras<4.0,>=3.0.0->tkan) (0.1.8)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from rich->keras<4.0,>=3.0.0->tkan) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from rich->keras<4.0,>=3.0.0->tkan) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anaconda\\envs\\dl\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras<4.0,>=3.0.0->tkan) (0.1.2)\n",
      "Downloading pyarrow-17.0.0-cp39-cp39-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/25.1 MB 390.1 kB/s eta 0:01:04\n",
      "    --------------------------------------- 0.5/25.1 MB 390.1 kB/s eta 0:01:04\n",
      "    --------------------------------------- 0.5/25.1 MB 390.1 kB/s eta 0:01:04\n",
      "   - -------------------------------------- 0.8/25.1 MB 441.3 kB/s eta 0:00:56\n",
      "   - -------------------------------------- 0.8/25.1 MB 441.3 kB/s eta 0:00:56\n",
      "   - -------------------------------------- 1.0/25.1 MB 465.9 kB/s eta 0:00:52\n",
      "   - -------------------------------------- 1.0/25.1 MB 465.9 kB/s eta 0:00:52\n",
      "   -- ------------------------------------- 1.3/25.1 MB 508.3 kB/s eta 0:00:47\n",
      "   -- ------------------------------------- 1.6/25.1 MB 541.2 kB/s eta 0:00:44\n",
      "   -- ------------------------------------- 1.6/25.1 MB 541.2 kB/s eta 0:00:44\n",
      "   -- ------------------------------------- 1.8/25.1 MB 568.6 kB/s eta 0:00:41\n",
      "   -- ------------------------------------- 1.8/25.1 MB 568.6 kB/s eta 0:00:41\n",
      "   --- ------------------------------------ 2.1/25.1 MB 593.1 kB/s eta 0:00:39\n",
      "   --- ------------------------------------ 2.4/25.1 MB 621.4 kB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 2.6/25.1 MB 656.5 kB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 679.3 kB/s eta 0:00:33\n",
      "   ----- ---------------------------------- 3.1/25.1 MB 709.7 kB/s eta 0:00:32\n",
      "   ----- ---------------------------------- 3.4/25.1 MB 726.8 kB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 3.7/25.1 MB 754.6 kB/s eta 0:00:29\n",
      "   ------ --------------------------------- 3.9/25.1 MB 775.2 kB/s eta 0:00:28\n",
      "   ------ --------------------------------- 4.2/25.1 MB 804.0 kB/s eta 0:00:27\n",
      "   ------- -------------------------------- 4.5/25.1 MB 828.4 kB/s eta 0:00:25\n",
      "   ------- -------------------------------- 5.0/25.1 MB 875.3 kB/s eta 0:00:24\n",
      "   -------- ------------------------------- 5.2/25.1 MB 897.9 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 5.8/25.1 MB 947.0 kB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 6.3/25.1 MB 994.4 kB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 6.6/25.1 MB 1.0 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 7.1/25.1 MB 1.1 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 7.6/25.1 MB 1.1 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 8.1/25.1 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 8.7/25.1 MB 1.2 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 9.2/25.1 MB 1.2 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 10.0/25.1 MB 1.3 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 10.5/25.1 MB 1.3 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 11.3/25.1 MB 1.4 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 11.8/25.1 MB 1.4 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 12.6/25.1 MB 1.5 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 13.1/25.1 MB 1.5 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 13.6/25.1 MB 1.5 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 14.2/25.1 MB 1.5 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 14.9/25.1 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 15.5/25.1 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 16.3/25.1 MB 1.7 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 16.8/25.1 MB 1.7 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 17.6/25.1 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 18.1/25.1 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 18.9/25.1 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 19.4/25.1 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 19.9/25.1 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.7/25.1 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 21.2/25.1 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 21.8/25.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.5/25.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.1/25.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.6/25.1 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.4/25.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading jax-0.4.21-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.7 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow, jax\n",
      "Successfully installed jax-0.4.21 pyarrow-17.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (d:\\anaconda\\envs\\dl\\lib\\site-packages)\n",
      "WARNING: jax 0.4.21 does not provide the extra 'cuda12'\n",
      "WARNING: Ignoring invalid distribution -umpy (d:\\anaconda\\envs\\dl\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (d:\\anaconda\\envs\\dl\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib pyarrow scikit-learn tkan \"jax[cuda12]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34213122-fabb-4d55-a918-a337be21b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BACKEND = 'torch' # You can use any backend here \n",
    "os.environ['KERAS_BACKEND'] = BACKEND\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# 创建目录（如果不存在）\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input, Flatten, GRU\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tkan import TKAN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95327e1-74a1-46d7-ac2a-90587eaf4685",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BatterySOCDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.features[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "def getData_ANN(train_files, test_file, batch_size, batch_size1, shuf):\n",
    "    # 读取训练集\n",
    "    train_data = pd.concat([pd.read_csv(file) for file in train_files], ignore_index=True)\n",
    "    train_data.drop('Profile', axis=1, inplace=True)\n",
    "\n",
    "    # 提取特征和标签\n",
    "    features = train_data.iloc[:, :3].values  # 除了最后一列SOC\n",
    "    labels = train_data.iloc[:, -1].values     # SOC作为标签\n",
    "\n",
    "    # 归一化特征\n",
    "    scaler = MinMaxScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    trainx, trainy = features, labels\n",
    "\n",
    "    # 读取测试集\n",
    "    test_data = pd.concat([pd.read_csv(file) for file in test_file], ignore_index=True)\n",
    "    test_data.drop('Profile', axis=1, inplace=True)\n",
    "    test_features = test_data.iloc[:, :3].values\n",
    "    test_labels = test_data.iloc[:, -1].values\n",
    "    test_features = scaler.transform(test_features)\n",
    "     # 为测试集特征添加0.5%的高斯白噪声\n",
    "    noise_std = 0.005 * np.std(test_features)  # 计算0.5%的标准差\n",
    "    # test_features = test_features + np.random.normal(0, noise_std, test_features.shape)\n",
    "\n",
    "    # 构建DataLoader\n",
    "    train_loader = DataLoader(dataset=BatterySOCDataset(trainx, trainy), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=BatterySOCDataset(test_features, test_labels), batch_size=batch_size1, shuffle=shuf)\n",
    "    \n",
    "\n",
    "    # SOC的最大最小值（用于反归一化）\n",
    "    close_max = labels.max()\n",
    "    close_min = labels.min()\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BatterySOCDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.features[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "def getData(train_files, test_file, batch_size, batch_size1, shuf, sequence):\n",
    "    # 读取训练集 CSV 文件\n",
    "    train_data = pd.concat([pd.read_csv(file) for file in train_files], ignore_index=True)\n",
    "\n",
    "    # 数据预处理\n",
    "    train_data.drop('Profile', axis=1, inplace=True)\n",
    "    close_max = train_data['SOC'].max()  # SOC的最大值\n",
    "    close_min = train_data['SOC'].min()  # SOC的最小值\n",
    "\n",
    "    # 提取特征和标签\n",
    "    features = train_data.iloc[:, :3].values  # 所有列除了最后一列\n",
    "    labels = train_data.iloc[:, -1].values  # 最后一列为 SOC 标签\n",
    "\n",
    "    # 归一化特征\n",
    "    scaler = MinMaxScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # 按文件划分序列\n",
    "    train_sequences = []\n",
    "    train_labels = []\n",
    "    \n",
    "    start_idx = 0\n",
    "    for file in train_files:\n",
    "        file_data = pd.read_csv(file)\n",
    "        file_data.drop('Profile', axis=1, inplace=True)\n",
    "\n",
    "        file_features = file_data.iloc[:, :3].values\n",
    "        file_labels = file_data.iloc[:, -1].values\n",
    "\n",
    "        # 归一化每个文件的特征\n",
    "        file_features = scaler.transform(file_features)\n",
    "\n",
    "        num_samples = len(file_features) - sequence+1\n",
    "        file_X = np.array([file_features[i:(i + sequence)] for i in range(0,num_samples)], dtype=np.float32)\n",
    "        file_Y = np.array([file_labels[i + sequence-1] for i in range(0,num_samples)], dtype=np.float32)\n",
    "        train_sequences.append(file_X)\n",
    "        train_labels.append(file_Y)\n",
    "\n",
    "    # 将所有文件的序列拼接到一起\n",
    "    X = np.concatenate(train_sequences, axis=0)\n",
    "    Y = np.concatenate(train_labels, axis=0)\n",
    "\n",
    "\n",
    "    # 划分训练集\n",
    "    total_len = len(Y)\n",
    "    trainx, trainy = X, Y  # 全部作为训练集\n",
    "\n",
    "    # 读取测试集 CSV 文件\n",
    "    test_data = pd.concat([pd.read_csv(file) for file in test_file], ignore_index=True)\n",
    "    test_data.drop('Profile', axis=1, inplace=True)\n",
    "\n",
    "    # 提取特征和标签\n",
    "    test_features = test_data.iloc[:, :3].values  # 所有列除了最后一列\n",
    "    test_labels = test_data.iloc[:, -1].values  # 最后一列为 SOC 标签\n",
    "\n",
    "    # 归一化测试集特征（使用训练集的缩放器）\n",
    "    test_features = scaler.transform(test_features)\n",
    "    \n",
    "    # 为测试集特征添加0.5%的高斯白噪声\n",
    "    noise_std = 0.005 * np.std(test_features)  # 计算0.5%的标准差\n",
    "    test_features_noisy = test_features + np.random.normal(0, noise_std, test_features.shape)\n",
    "    \n",
    "    # 按文件划分序列\n",
    "    test_sequences = []\n",
    "    test_labels_final = []\n",
    "    \n",
    "    for file in test_file:\n",
    "        file_data = pd.read_csv(file)\n",
    "        file_data.drop('Profile', axis=1, inplace=True)\n",
    "\n",
    "        file_features = file_data.iloc[:, :3].values\n",
    "        file_labels = file_data.iloc[:, -1].values\n",
    "\n",
    "        # 归一化每个文件的特征\n",
    "        file_features = scaler.transform(file_features)\n",
    "        \n",
    "        # 为每个文件的测试数据添加相同的噪声比例\n",
    "        file_features = file_features + np.random.normal(0, noise_std, file_features.shape)\n",
    "\n",
    "        num_samples = len(file_features) - sequence+1\n",
    "        file_X = np.array([file_features[i:(i + sequence)] for i in range(0,num_samples)], dtype=np.float32)\n",
    "        file_Y = np.array([file_labels[i + sequence-1] for i in range(0,num_samples)], dtype=np.float32)\n",
    "        test_sequences.append(file_X)\n",
    "        test_labels_final.append(file_Y)\n",
    "\n",
    "    # 将所有文件的序列拼接到一起\n",
    "    test_X = np.concatenate(test_sequences, axis=0)\n",
    "    test_Y = np.concatenate(test_labels_final, axis=0)\n",
    "\n",
    "    # 构建DataLoader，用于批次训练\n",
    "    train_loader = DataLoader(dataset=BatterySOCDataset(trainx, trainy), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=BatterySOCDataset(test_X, test_Y), batch_size=batch_size1, shuffle=shuf)\n",
    "    \n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c12726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BatterySOCDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.features[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "def getData(train_files, test_file, batch_size, batch_size1, shuf, sequence):\n",
    "    # 读取训练集 CSV 文件\n",
    "    train_data = pd.concat([pd.read_csv(file) for file in train_files], ignore_index=True)\n",
    "\n",
    "    # 数据预处理\n",
    "    train_data.drop('Profile', axis=1, inplace=True)\n",
    "    close_max = train_data['SOC'].max()  # SOC的最大值\n",
    "    close_min = train_data['SOC'].min()  # SOC的最小值\n",
    "\n",
    "    # 提取特征和标签\n",
    "    features = train_data.iloc[:, :3].values  # 所有列除了最后一列\n",
    "    labels = train_data.iloc[:, -1].values  # 最后一列为 SOC 标签\n",
    "\n",
    "    # 归一化特征\n",
    "    scaler = MinMaxScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # 按文件划分序列\n",
    "    train_sequences = []\n",
    "    train_labels = []\n",
    "    \n",
    "    start_idx = 0\n",
    "    for file in train_files:\n",
    "        file_data = pd.read_csv(file)\n",
    "        file_data.drop('Profile', axis=1, inplace=True)\n",
    "\n",
    "        file_features = file_data.iloc[:, :3].values\n",
    "        file_labels = file_data.iloc[:, -1].values\n",
    "\n",
    "        # 归一化每个文件的特征\n",
    "        file_features = scaler.transform(file_features)\n",
    "\n",
    "        num_samples = len(file_features) - sequence+1\n",
    "        file_X = np.array([file_features[i:(i + sequence)] for i in range(0,num_samples)], dtype=np.float32)\n",
    "        file_Y = np.array([file_labels[i + sequence-1] for i in range(0,num_samples)], dtype=np.float32)\n",
    "        train_sequences.append(file_X)\n",
    "        train_labels.append(file_Y)\n",
    "\n",
    "    # 将所有文件的序列拼接到一起\n",
    "    X = np.concatenate(train_sequences, axis=0)\n",
    "    Y = np.concatenate(train_labels, axis=0)\n",
    "\n",
    "    # 划分训练集\n",
    "    trainx, trainy = X, Y  # 全部作为训练集\n",
    "\n",
    "    # 读取测试集 CSV 文件\n",
    "    test_data = pd.concat([pd.read_csv(file) for file in test_file], ignore_index=True)\n",
    "    test_data.drop('Profile', axis=1, inplace=True)\n",
    "    \n",
    "    # 按文件划分序列\n",
    "    test_sequences = []\n",
    "    test_labels_final = []\n",
    "    \n",
    "    for file in test_file:\n",
    "        file_data = pd.read_csv(file)\n",
    "        file_data.drop('Profile', axis=1, inplace=True)\n",
    "\n",
    "        file_features = file_data.iloc[:, :3].values\n",
    "        file_labels = file_data.iloc[:, -1].values\n",
    "\n",
    "        # 归一化每个文件的特征\n",
    "        file_features = scaler.transform(file_features)\n",
    "\n",
    "        num_samples = len(file_features) - sequence+1\n",
    "        file_X = np.array([file_features[i:(i + sequence)] for i in range(0,num_samples)], dtype=np.float32)\n",
    "        file_Y = np.array([file_labels[i + sequence-1] for i in range(0,num_samples)], dtype=np.float32)\n",
    "        test_sequences.append(file_X)\n",
    "        test_labels_final.append(file_Y)\n",
    "\n",
    "    # 将所有文件的序列拼接到一起\n",
    "    test_X = np.concatenate(test_sequences, axis=0)\n",
    "    test_Y = np.concatenate(test_labels_final, axis=0)\n",
    "\n",
    "    # 构建DataLoader，用于批次训练\n",
    "    train_loader = DataLoader(dataset=BatterySOCDataset(trainx, trainy), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=BatterySOCDataset(test_X, test_Y), batch_size=batch_size1, shuffle=shuf)\n",
    "    \n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2335ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 92,201\n",
      "Epoch 1, Loss: 6.1860817316919565\n",
      "Epoch 1, rmse: 0.25325822830200195\n",
      "Epoch 1, Test Loss: 0.056337080895900726\n",
      "New best model saved.\n",
      "Epoch 2, Loss: 3.0886808382347226\n",
      "Epoch 2, rmse: 0.11826898902654648\n",
      "Epoch 2, Test Loss: 0.01626112125813961\n",
      "New best model saved.\n",
      "Epoch 3, Loss: 0.6191336591728032\n",
      "Epoch 3, rmse: 0.06458527594804764\n",
      "Epoch 3, Test Loss: 0.0033097960986196995\n",
      "New best model saved.\n",
      "Epoch 4, Loss: 0.3198437348473817\n",
      "Epoch 4, rmse: 0.053895093500614166\n",
      "Epoch 4, Test Loss: 0.0023021171800792217\n",
      "New best model saved.\n",
      "Epoch 5, Loss: 0.306311103515327\n",
      "Epoch 5, rmse: 0.05128130316734314\n",
      "Epoch 5, Test Loss: 0.0021797558292746544\n",
      "New best model saved.\n",
      "Epoch 6, Loss: 0.31606109580025077\n",
      "Epoch 6, rmse: 0.05161287635564804\n",
      "Epoch 6, Test Loss: 0.0027967977803200483\n",
      "Epoch 7, Loss: 0.2514874304179102\n",
      "Epoch 7, rmse: 0.052867770195007324\n",
      "Epoch 7, Test Loss: 0.0027920901775360107\n",
      "Epoch 8, Loss: 0.27260532369837165\n",
      "Epoch 8, rmse: 0.047577112913131714\n",
      "Epoch 8, Test Loss: 0.0019656529184430838\n",
      "New best model saved.\n",
      "Epoch 9, Loss: 0.25477514532394707\n",
      "Epoch 9, rmse: 0.04014338552951813\n",
      "Epoch 9, Test Loss: 0.002011011354625225\n",
      "Epoch 10, Loss: 0.23261836683377624\n",
      "Epoch 10, rmse: 0.04760744050145149\n",
      "Epoch 10, Test Loss: 0.0028864024206995964\n",
      "Epoch 11, Loss: 0.17009099666029215\n",
      "Epoch 11, rmse: 0.040126342326402664\n",
      "Epoch 11, Test Loss: 0.002313042990863323\n",
      "Epoch 12, Loss: 0.18142637831624597\n",
      "Epoch 12, rmse: 0.04894629120826721\n",
      "Epoch 12, Test Loss: 0.0024370441678911448\n",
      "Epoch 13, Loss: 0.14878204668639228\n",
      "Epoch 13, rmse: 0.030122892931103706\n",
      "Epoch 13, Test Loss: 0.0022446364164352417\n",
      "Epoch 14, Loss: 0.1425501472549513\n",
      "Epoch 14, rmse: 0.036193400621414185\n",
      "Epoch 14, Test Loss: 0.003186503192409873\n",
      "Epoch 15, Loss: 0.13764793623704463\n",
      "Epoch 15, rmse: 0.042018573731184006\n",
      "Epoch 15, Test Loss: 0.004012593999505043\n",
      "Epoch 16, Loss: 0.13069425203138962\n",
      "Epoch 16, rmse: 0.028508014976978302\n",
      "Epoch 16, Test Loss: 0.0020290000829845667\n",
      "Epoch 17, Loss: 0.10482999490341172\n",
      "Epoch 17, rmse: 0.03788291662931442\n",
      "Epoch 17, Test Loss: 0.0021712968591600657\n",
      "Epoch 18, Loss: 0.12714048201451078\n",
      "Epoch 18, rmse: 0.036252059042453766\n",
      "Epoch 18, Test Loss: 0.0031830170191824436\n",
      "Epoch 19, Loss: 0.126482097490225\n",
      "Epoch 19, rmse: 0.028804797679185867\n",
      "Epoch 19, Test Loss: 0.0021988924127072096\n",
      "Epoch 20, Loss: 0.1171824872144498\n",
      "Epoch 20, rmse: 0.030551020056009293\n",
      "Epoch 20, Test Loss: 0.0019226375734433532\n",
      "New best model saved.\n",
      "Epoch 21, Loss: 0.08803574327612296\n",
      "Epoch 21, rmse: 0.0394677110016346\n",
      "Epoch 21, Test Loss: 0.002948212902992964\n",
      "Epoch 22, Loss: 0.08618552319239825\n",
      "Epoch 22, rmse: 0.026112183928489685\n",
      "Epoch 22, Test Loss: 0.0020626161713153124\n",
      "Epoch 23, Loss: 0.09556154918391258\n",
      "Epoch 23, rmse: 0.028088249266147614\n",
      "Epoch 23, Test Loss: 0.0019996841438114643\n",
      "Epoch 24, Loss: 0.11690969479968771\n",
      "Epoch 24, rmse: 0.02972947619855404\n",
      "Epoch 24, Test Loss: 0.002357680117711425\n",
      "Epoch 25, Loss: 0.08985517465043813\n",
      "Epoch 25, rmse: 0.032034676522016525\n",
      "Epoch 25, Test Loss: 0.0017983189318329096\n",
      "New best model saved.\n",
      "Epoch 26, Loss: 0.10169952979777008\n",
      "Epoch 26, rmse: 0.02574208937585354\n",
      "Epoch 26, Test Loss: 0.0018508940702304244\n",
      "Epoch 27, Loss: 0.10362608137074858\n",
      "Epoch 27, rmse: 0.030422674492001534\n",
      "Epoch 27, Test Loss: 0.0018648910336196423\n",
      "Epoch 28, Loss: 0.08121006254805252\n",
      "Epoch 28, rmse: 0.039558980613946915\n",
      "Epoch 28, Test Loss: 0.001746034249663353\n",
      "New best model saved.\n",
      "Epoch 29, Loss: 0.0932586319395341\n",
      "Epoch 29, rmse: 0.02820698730647564\n",
      "Epoch 29, Test Loss: 0.0028798009734600782\n",
      "Epoch 30, Loss: 0.09009225683985278\n",
      "Epoch 30, rmse: 0.0312103983014822\n",
      "Epoch 30, Test Loss: 0.002120439661666751\n",
      "Epoch 31, Loss: 0.07061177713330835\n",
      "Epoch 31, rmse: 0.035041309893131256\n",
      "Epoch 31, Test Loss: 0.0019367202185094357\n",
      "Epoch 32, Loss: 0.0746651294757612\n",
      "Epoch 32, rmse: 0.03092164546251297\n",
      "Epoch 32, Test Loss: 0.001593198860064149\n",
      "New best model saved.\n",
      "Epoch 33, Loss: 0.07017136458307505\n",
      "Epoch 33, rmse: 0.022396191954612732\n",
      "Epoch 33, Test Loss: 0.0018047487828880548\n",
      "Epoch 34, Loss: 0.07833920582197607\n",
      "Epoch 34, rmse: 0.024344120174646378\n",
      "Epoch 34, Test Loss: 0.0016429583774879575\n",
      "Epoch 35, Loss: 0.08894748950842768\n",
      "Epoch 35, rmse: 0.024529261514544487\n",
      "Epoch 35, Test Loss: 0.002248606411740184\n",
      "Epoch 36, Loss: 0.07016416775877587\n",
      "Epoch 36, rmse: 0.023673174902796745\n",
      "Epoch 36, Test Loss: 0.0016697122482582927\n",
      "Epoch 37, Loss: 0.09432079369435087\n",
      "Epoch 37, rmse: 0.029047464951872826\n",
      "Epoch 37, Test Loss: 0.0017198494169861078\n",
      "Epoch 38, Loss: 0.07055774459149688\n",
      "Epoch 38, rmse: 0.023677928373217583\n",
      "Epoch 38, Test Loss: 0.0017645953921601176\n",
      "Epoch 39, Loss: 0.0742819665465504\n",
      "Epoch 39, rmse: 0.021729296073317528\n",
      "Epoch 39, Test Loss: 0.0015348431188613176\n",
      "New best model saved.\n",
      "Epoch 40, Loss: 0.07453749113483354\n",
      "Epoch 40, rmse: 0.03668442741036415\n",
      "Epoch 40, Test Loss: 0.0017812633886933327\n",
      "Epoch 41, Loss: 0.061321765097090974\n",
      "Epoch 41, rmse: 0.02790561504662037\n",
      "Epoch 41, Test Loss: 0.0020571439526975155\n",
      "Epoch 42, Loss: 0.056804572901455685\n",
      "Epoch 42, rmse: 0.025685537606477737\n",
      "Epoch 42, Test Loss: 0.0017405045218765736\n",
      "Epoch 43, Loss: 0.06023762235417962\n",
      "Epoch 43, rmse: 0.020541904494166374\n",
      "Epoch 43, Test Loss: 0.0021071049850434065\n",
      "Epoch 44, Loss: 0.08513204581686296\n",
      "Epoch 44, rmse: 0.02069105952978134\n",
      "Epoch 44, Test Loss: 0.0016108177369460464\n",
      "Epoch 45, Loss: 0.0506019952299539\n",
      "Epoch 45, rmse: 0.021258492022752762\n",
      "Epoch 45, Test Loss: 0.0017254753038287163\n",
      "Epoch 46, Loss: 0.06291774427518249\n",
      "Epoch 46, rmse: 0.023724006488919258\n",
      "Epoch 46, Test Loss: 0.0022818504367023706\n",
      "Epoch 47, Loss: 0.05884128512116149\n",
      "Epoch 47, rmse: 0.021516362205147743\n",
      "Epoch 47, Test Loss: 0.002157794777303934\n",
      "Epoch 48, Loss: 0.07491712985211052\n",
      "Epoch 48, rmse: 0.021423447877168655\n",
      "Epoch 48, Test Loss: 0.0017841198714450002\n",
      "Epoch 49, Loss: 0.05226808600127697\n",
      "Epoch 49, rmse: 0.018958868458867073\n",
      "Epoch 49, Test Loss: 0.0017096991650760174\n",
      "Epoch 50, Loss: 0.05057969197514467\n",
      "Epoch 50, rmse: 0.01909874565899372\n",
      "Epoch 50, Test Loss: 0.0018844323931261897\n",
      "Epoch 51, Loss: 0.04120272572617978\n",
      "Epoch 51, rmse: 0.019512513652443886\n",
      "Epoch 51, Test Loss: 0.0018244147067889571\n",
      "Epoch 52, Loss: 0.05031741733546369\n",
      "Epoch 52, rmse: 0.018758663907647133\n",
      "Epoch 52, Test Loss: 0.0017403893871232867\n",
      "Epoch 53, Loss: 0.05093351207324304\n",
      "Epoch 53, rmse: 0.030286051332950592\n",
      "Epoch 53, Test Loss: 0.004395810887217522\n",
      "Epoch 54, Loss: 0.06084592192200944\n",
      "Epoch 54, rmse: 0.023624693974852562\n",
      "Epoch 54, Test Loss: 0.00280366069637239\n",
      "Epoch 55, Loss: 0.043458430329337716\n",
      "Epoch 55, rmse: 0.017358878627419472\n",
      "Epoch 55, Test Loss: 0.0018456758698448539\n",
      "Epoch 56, Loss: 0.03300167975248769\n",
      "Epoch 56, rmse: 0.0190202035009861\n",
      "Epoch 56, Test Loss: 0.0016369960503652692\n",
      "Epoch 57, Loss: 0.045222163083963096\n",
      "Epoch 57, rmse: 0.01728387176990509\n",
      "Epoch 57, Test Loss: 0.0016253232024610043\n",
      "Epoch 58, Loss: 0.04734820523299277\n",
      "Epoch 58, rmse: 0.016779562458395958\n",
      "Epoch 58, Test Loss: 0.0017250871751457453\n",
      "Epoch 59, Loss: 0.05979023591498844\n",
      "Epoch 59, rmse: 0.022894229739904404\n",
      "Epoch 59, Test Loss: 0.0017851702868938446\n",
      "Epoch 60, Loss: 0.054427439405117184\n",
      "Epoch 60, rmse: 0.023958835750818253\n",
      "Epoch 60, Test Loss: 0.0020814447198063135\n",
      "Epoch 61, Loss: 0.03243377529724967\n",
      "Epoch 61, rmse: 0.01653790846467018\n",
      "Epoch 61, Test Loss: 0.0018660796340554953\n",
      "Epoch 62, Loss: 0.0330819487280678\n",
      "Epoch 62, rmse: 0.021311793476343155\n",
      "Epoch 62, Test Loss: 0.001906073186546564\n",
      "Epoch 63, Loss: 0.03776684473268688\n",
      "Epoch 63, rmse: 0.02065163664519787\n",
      "Epoch 63, Test Loss: 0.001791608170606196\n",
      "Epoch 64, Loss: 0.03425608230463695\n",
      "Epoch 64, rmse: 0.018244745209813118\n",
      "Epoch 64, Test Loss: 0.0016738434787839651\n",
      "Epoch 65, Loss: 0.0361067000485491\n",
      "Epoch 65, rmse: 0.01930341310799122\n",
      "Epoch 65, Test Loss: 0.0026117661036551\n",
      "Epoch 66, Loss: 0.033316287022898905\n",
      "Epoch 66, rmse: 0.01577388308942318\n",
      "Epoch 66, Test Loss: 0.0018288051942363381\n",
      "Epoch 67, Loss: 0.03224742808379233\n",
      "Epoch 67, rmse: 0.021716760471463203\n",
      "Epoch 67, Test Loss: 0.0016640240792185068\n",
      "Epoch 68, Loss: 0.03279852333071176\n",
      "Epoch 68, rmse: 0.03303904086351395\n",
      "Epoch 68, Test Loss: 0.003254241542890668\n",
      "Epoch 69, Loss: 0.05193537153536454\n",
      "Epoch 69, rmse: 0.01530230138450861\n",
      "Epoch 69, Test Loss: 0.002027370734140277\n",
      "Epoch 70, Loss: 0.0311759091418935\n",
      "Epoch 70, rmse: 0.023138603195548058\n",
      "Epoch 70, Test Loss: 0.0017314930446445942\n",
      "Epoch 71, Loss: 0.026219039558782242\n",
      "Epoch 71, rmse: 0.015611836686730385\n",
      "Epoch 71, Test Loss: 0.0020190582145005465\n",
      "Epoch 72, Loss: 0.027408766283770092\n",
      "Epoch 72, rmse: 0.01527061965316534\n",
      "Epoch 72, Test Loss: 0.0019773223903030157\n",
      "Epoch 73, Loss: 0.030078474839683622\n",
      "Epoch 73, rmse: 0.01527048647403717\n",
      "Epoch 73, Test Loss: 0.0019619695376604795\n",
      "Epoch 74, Loss: 0.023859987035393715\n",
      "Epoch 74, rmse: 0.019982796162366867\n",
      "Epoch 74, Test Loss: 0.002337459707632661\n",
      "Epoch 75, Loss: 0.03402791652479209\n",
      "Epoch 75, rmse: 0.012977149337530136\n",
      "Epoch 75, Test Loss: 0.0017223887844011188\n",
      "Epoch 76, Loss: 0.03091310543823056\n",
      "Epoch 76, rmse: 0.015467372722923756\n",
      "Epoch 76, Test Loss: 0.002104668179526925\n",
      "Epoch 77, Loss: 0.02400521220988594\n",
      "Epoch 77, rmse: 0.025203784927725792\n",
      "Epoch 77, Test Loss: 0.003112073289230466\n",
      "Epoch 78, Loss: 0.036508732722722925\n",
      "Epoch 78, rmse: 0.027730129659175873\n",
      "Epoch 78, Test Loss: 0.0015509608201682568\n",
      "Epoch 79, Loss: 0.026060304837301373\n",
      "Epoch 79, rmse: 0.013087055645883083\n",
      "Epoch 79, Test Loss: 0.0017631686059758067\n",
      "Epoch 80, Loss: 0.02959892801300157\n",
      "Epoch 80, rmse: 0.015823200345039368\n",
      "Epoch 80, Test Loss: 0.0021867232862859964\n",
      "Epoch 81, Loss: 0.02643179388542194\n",
      "Epoch 81, rmse: 0.013825379312038422\n",
      "Epoch 81, Test Loss: 0.001849064719863236\n",
      "Epoch 82, Loss: 0.019154220644850284\n",
      "Epoch 82, rmse: 0.015531695447862148\n",
      "Epoch 82, Test Loss: 0.0016901235794648528\n",
      "Epoch 83, Loss: 0.02857062242401298\n",
      "Epoch 83, rmse: 0.016189828515052795\n",
      "Epoch 83, Test Loss: 0.0020444111432880163\n",
      "Epoch 84, Loss: 0.01987922597618308\n",
      "Epoch 84, rmse: 0.014700637198984623\n",
      "Epoch 84, Test Loss: 0.0016330438666045666\n",
      "Epoch 85, Loss: 0.027342364337528124\n",
      "Epoch 85, rmse: 0.013413372449576855\n",
      "Epoch 85, Test Loss: 0.0017298688180744648\n",
      "Epoch 86, Loss: 0.030661964643513784\n",
      "Epoch 86, rmse: 0.014106100425124168\n",
      "Epoch 86, Test Loss: 0.0018860048148781061\n",
      "Epoch 87, Loss: 0.022666505166853312\n",
      "Epoch 87, rmse: 0.013444716110825539\n",
      "Epoch 87, Test Loss: 0.0018999119056388736\n",
      "Epoch 88, Loss: 0.02700898532930296\n",
      "Epoch 88, rmse: 0.01624886691570282\n",
      "Epoch 88, Test Loss: 0.002180123934522271\n",
      "Epoch 89, Loss: 0.020588124374626204\n",
      "Epoch 89, rmse: 0.013603578321635723\n",
      "Epoch 89, Test Loss: 0.0018665057141333818\n",
      "Epoch 90, Loss: 0.02286309479677584\n",
      "Epoch 90, rmse: 0.01656433381140232\n",
      "Epoch 90, Test Loss: 0.0017042503459379077\n",
      "Epoch 91, Loss: 0.01669060132553568\n",
      "Epoch 91, rmse: 0.012493852525949478\n",
      "Epoch 91, Test Loss: 0.0017361125210300088\n",
      "Epoch 92, Loss: 0.015875125216552988\n",
      "Epoch 92, rmse: 0.015619444660842419\n",
      "Epoch 92, Test Loss: 0.0016202547121793032\n",
      "Epoch 93, Loss: 0.01897260077384999\n",
      "Epoch 93, rmse: 0.01298456359654665\n",
      "Epoch 93, Test Loss: 0.001808467204682529\n",
      "Epoch 94, Loss: 0.020244166313204914\n",
      "Epoch 94, rmse: 0.011216088198125362\n",
      "Epoch 94, Test Loss: 0.001882346230559051\n",
      "Epoch 95, Loss: 0.018790250906022266\n",
      "Epoch 95, rmse: 0.012798995710909367\n",
      "Epoch 95, Test Loss: 0.0022273187059909105\n",
      "Epoch 96, Loss: 0.01744555741606746\n",
      "Epoch 96, rmse: 0.012989812530577183\n",
      "Epoch 96, Test Loss: 0.0018338283989578485\n",
      "Epoch 97, Loss: 0.01806880618823925\n",
      "Epoch 97, rmse: 0.014775288291275501\n",
      "Epoch 97, Test Loss: 0.0022397038992494345\n",
      "Epoch 98, Loss: 0.018124392961908597\n",
      "Epoch 98, rmse: 0.017143841832876205\n",
      "Epoch 98, Test Loss: 0.0021199039183557034\n",
      "Epoch 99, Loss: 0.02187712055456359\n",
      "Epoch 99, rmse: 0.01924828253686428\n",
      "Epoch 99, Test Loss: 0.002040903316810727\n",
      "Epoch 100, Loss: 0.02458984829718247\n",
      "Epoch 100, rmse: 0.012174958363175392\n",
      "Epoch 100, Test Loss: 0.001846594619564712\n",
      "Epoch 101, Loss: 0.012876034306827933\n",
      "Epoch 101, rmse: 0.010661874897778034\n",
      "Epoch 101, Test Loss: 0.00178793934173882\n",
      "Epoch 102, Loss: 0.014155459706671536\n",
      "Epoch 102, rmse: 0.01509234681725502\n",
      "Epoch 102, Test Loss: 0.0021304849069565535\n",
      "Epoch 103, Loss: 0.01719314378715353\n",
      "Epoch 103, rmse: 0.010968470014631748\n",
      "Epoch 103, Test Loss: 0.001920510083436966\n",
      "Epoch 104, Loss: 0.014620033216488082\n",
      "Epoch 104, rmse: 0.010545478202402592\n",
      "Epoch 104, Test Loss: 0.0020139971747994423\n",
      "Epoch 105, Loss: 0.015152799162024166\n",
      "Epoch 105, rmse: 0.019057651981711388\n",
      "Epoch 105, Test Loss: 0.002565259113907814\n",
      "Epoch 106, Loss: 0.017945605621207505\n",
      "Epoch 106, rmse: 0.009728717617690563\n",
      "Epoch 106, Test Loss: 0.0018663369119167328\n",
      "Epoch 107, Loss: 0.01644562435831176\n",
      "Epoch 107, rmse: 0.00959055870771408\n",
      "Epoch 107, Test Loss: 0.0018955389969050884\n",
      "Epoch 108, Loss: 0.01495937770960154\n",
      "Epoch 108, rmse: 0.014209327287971973\n",
      "Epoch 108, Test Loss: 0.0016704718582332134\n",
      "Epoch 109, Loss: 0.018252057227073237\n",
      "Epoch 109, rmse: 0.014049929566681385\n",
      "Epoch 109, Test Loss: 0.0018134713172912598\n",
      "Epoch 110, Loss: 0.0207179963690578\n",
      "Epoch 110, rmse: 0.0170137956738472\n",
      "Epoch 110, Test Loss: 0.0019637206569314003\n",
      "Epoch 111, Loss: 0.012843440861615818\n",
      "Epoch 111, rmse: 0.012158484198153019\n",
      "Epoch 111, Test Loss: 0.0019746317993849516\n",
      "Epoch 112, Loss: 0.009951027524948586\n",
      "Epoch 112, rmse: 0.009009070694446564\n",
      "Epoch 112, Test Loss: 0.0019231020705774426\n",
      "Epoch 113, Loss: 0.014160528080537915\n",
      "Epoch 113, rmse: 0.022781042382121086\n",
      "Epoch 113, Test Loss: 0.0019826143980026245\n",
      "Epoch 114, Loss: 0.01380787569360109\n",
      "Epoch 114, rmse: 0.012970124371349812\n",
      "Epoch 114, Test Loss: 0.0017574591329321265\n",
      "Epoch 115, Loss: 0.013328345317859203\n",
      "Epoch 115, rmse: 0.009046957828104496\n",
      "Epoch 115, Test Loss: 0.00190802535507828\n",
      "Epoch 116, Loss: 0.01756858803128125\n",
      "Epoch 116, rmse: 0.010191062465310097\n",
      "Epoch 116, Test Loss: 0.0016632521292194724\n",
      "Epoch 117, Loss: 0.01214906558743678\n",
      "Epoch 117, rmse: 0.009251756593585014\n",
      "Epoch 117, Test Loss: 0.0019290451891720295\n",
      "Epoch 118, Loss: 0.013883936335332692\n",
      "Epoch 118, rmse: 0.01855025254189968\n",
      "Epoch 118, Test Loss: 0.0019133384339511395\n",
      "Epoch 119, Loss: 0.016139103943714872\n",
      "Epoch 119, rmse: 0.010074314661324024\n",
      "Epoch 119, Test Loss: 0.0018102711765095592\n",
      "Epoch 120, Loss: 0.012308627112361137\n",
      "Epoch 120, rmse: 0.00955280289053917\n",
      "Epoch 120, Test Loss: 0.00184068048838526\n",
      "Epoch 121, Loss: 0.00887292149127461\n",
      "Epoch 121, rmse: 0.012757058255374432\n",
      "Epoch 121, Test Loss: 0.0017506041331216693\n",
      "Epoch 122, Loss: 0.010236997331958264\n",
      "Epoch 122, rmse: 0.00830510351806879\n",
      "Epoch 122, Test Loss: 0.0018763894913718104\n",
      "Epoch 123, Loss: 0.010195426177233458\n",
      "Epoch 123, rmse: 0.008948433212935925\n",
      "Epoch 123, Test Loss: 0.0018241850193589926\n",
      "Epoch 124, Loss: 0.011204530150280334\n",
      "Epoch 124, rmse: 0.010648952797055244\n",
      "Epoch 124, Test Loss: 0.0017769640544429421\n",
      "Epoch 125, Loss: 0.01322491234896006\n",
      "Epoch 125, rmse: 0.008721676655113697\n",
      "Epoch 125, Test Loss: 0.0017589546041563153\n",
      "Epoch 126, Loss: 0.011538132683199365\n",
      "Epoch 126, rmse: 0.008868935517966747\n",
      "Epoch 126, Test Loss: 0.0019205001881346107\n",
      "Epoch 127, Loss: 0.011076613816840108\n",
      "Epoch 127, rmse: 0.01201936136931181\n",
      "Epoch 127, Test Loss: 0.0018452026415616274\n",
      "Epoch 128, Loss: 0.011839952916488983\n",
      "Epoch 128, rmse: 0.008326352573931217\n",
      "Epoch 128, Test Loss: 0.0018512926762923598\n",
      "Epoch 129, Loss: 0.013294619515363593\n",
      "Epoch 129, rmse: 0.008815241977572441\n",
      "Epoch 129, Test Loss: 0.0018700072541832924\n",
      "Epoch 130, Loss: 0.012027733006107155\n",
      "Epoch 130, rmse: 0.012452957220375538\n",
      "Epoch 130, Test Loss: 0.0019851704128086567\n",
      "Epoch 131, Loss: 0.008235474288085243\n",
      "Epoch 131, rmse: 0.007421757094562054\n",
      "Epoch 131, Test Loss: 0.0018509194487705827\n",
      "Epoch 132, Loss: 0.00863122472219402\n",
      "Epoch 132, rmse: 0.009181912057101727\n",
      "Epoch 132, Test Loss: 0.0017968618776649237\n",
      "Epoch 133, Loss: 0.010042173889814876\n",
      "Epoch 133, rmse: 0.008951619267463684\n",
      "Epoch 133, Test Loss: 0.001817312790080905\n",
      "Epoch 134, Loss: 0.009518650273093954\n",
      "Epoch 134, rmse: 0.016441015526652336\n",
      "Epoch 134, Test Loss: 0.002298555104061961\n",
      "Epoch 135, Loss: 0.011959301453316584\n",
      "Epoch 135, rmse: 0.010054254904389381\n",
      "Epoch 135, Test Loss: 0.0017814935417845845\n",
      "Epoch 136, Loss: 0.009570217836881056\n",
      "Epoch 136, rmse: 0.009185031987726688\n",
      "Epoch 136, Test Loss: 0.0018788607558235526\n",
      "Epoch 137, Loss: 0.010648618826962775\n",
      "Epoch 137, rmse: 0.01169952005147934\n",
      "Epoch 137, Test Loss: 0.0020520640537142754\n",
      "Epoch 138, Loss: 0.011029929628421087\n",
      "Epoch 138, rmse: 0.008503684774041176\n",
      "Epoch 138, Test Loss: 0.0019296160899102688\n",
      "Epoch 139, Loss: 0.010671346972230822\n",
      "Epoch 139, rmse: 0.008081351406872272\n",
      "Epoch 139, Test Loss: 0.001797746866941452\n",
      "Epoch 140, Loss: 0.012157231423771009\n",
      "Epoch 140, rmse: 0.013606082648038864\n",
      "Epoch 140, Test Loss: 0.0019587762653827667\n",
      "Epoch 141, Loss: 0.007397892066364875\n",
      "Epoch 141, rmse: 0.008188420906662941\n",
      "Epoch 141, Test Loss: 0.001936442800797522\n",
      "Epoch 142, Loss: 0.006690732687275158\n",
      "Epoch 142, rmse: 0.007810778450220823\n",
      "Epoch 142, Test Loss: 0.0018388173775747418\n",
      "Epoch 143, Loss: 0.008267168708698591\n",
      "Epoch 143, rmse: 0.008100326173007488\n",
      "Epoch 143, Test Loss: 0.0018346618162468076\n",
      "Epoch 144, Loss: 0.007486905880796257\n",
      "Epoch 144, rmse: 0.007782974746078253\n",
      "Epoch 144, Test Loss: 0.0018823874415829778\n",
      "Epoch 145, Loss: 0.008247107984061586\n",
      "Epoch 145, rmse: 0.01029474101960659\n",
      "Epoch 145, Test Loss: 0.0018588470993563533\n",
      "Epoch 146, Loss: 0.00971728251533932\n",
      "Epoch 146, rmse: 0.009164269082248211\n",
      "Epoch 146, Test Loss: 0.0019495983142405748\n",
      "Epoch 147, Loss: 0.009369368712214055\n",
      "Epoch 147, rmse: 0.009457021951675415\n",
      "Epoch 147, Test Loss: 0.0020590496715158224\n",
      "Epoch 148, Loss: 0.010146039985556854\n",
      "Epoch 148, rmse: 0.010603610426187515\n",
      "Epoch 148, Test Loss: 0.0018430891213938594\n",
      "Epoch 149, Loss: 0.009122457995545119\n",
      "Epoch 149, rmse: 0.013627472333610058\n",
      "Epoch 149, Test Loss: 0.001885205740109086\n",
      "Epoch 150, Loss: 0.009221506934409263\n",
      "Epoch 150, rmse: 0.007740573957562447\n",
      "Epoch 150, Test Loss: 0.001967768417671323\n",
      "Epoch 151, Loss: 0.0062451982412312645\n",
      "Epoch 151, rmse: 0.007369266357272863\n",
      "Epoch 151, Test Loss: 0.0018359647365286946\n",
      "Epoch 152, Loss: 0.006805952569266083\n",
      "Epoch 152, rmse: 0.007083140779286623\n",
      "Epoch 152, Test Loss: 0.0017934408970177174\n",
      "Epoch 153, Loss: 0.007093499105394585\n",
      "Epoch 153, rmse: 0.007835046388208866\n",
      "Epoch 153, Test Loss: 0.001827196916565299\n",
      "Epoch 154, Loss: 0.007274715499079321\n",
      "Epoch 154, rmse: 0.007600341457873583\n",
      "Epoch 154, Test Loss: 0.0019349980866536498\n",
      "Epoch 155, Loss: 0.008804736648016842\n",
      "Epoch 155, rmse: 0.010069535113871098\n",
      "Epoch 155, Test Loss: 0.0019136386690661311\n",
      "Epoch 156, Loss: 0.006700338977680076\n",
      "Epoch 156, rmse: 0.009823398664593697\n",
      "Epoch 156, Test Loss: 0.0018543057376518846\n",
      "Epoch 157, Loss: 0.007847468012187164\n",
      "Epoch 157, rmse: 0.012911383993923664\n",
      "Epoch 157, Test Loss: 0.002043378772214055\n",
      "Epoch 158, Loss: 0.00683352899068268\n",
      "Epoch 158, rmse: 0.008046098053455353\n",
      "Epoch 158, Test Loss: 0.0018424916779622436\n",
      "Epoch 159, Loss: 0.008961601775808958\n",
      "Epoch 159, rmse: 0.011781256645917892\n",
      "Epoch 159, Test Loss: 0.0018975514685735106\n",
      "Epoch 160, Loss: 0.006131152149464469\n",
      "Epoch 160, rmse: 0.007765889633446932\n",
      "Epoch 160, Test Loss: 0.0018620487535372376\n",
      "Epoch 161, Loss: 0.006690141817671247\n",
      "Epoch 161, rmse: 0.006880340166389942\n",
      "Epoch 161, Test Loss: 0.0018903672462329268\n",
      "Epoch 162, Loss: 0.005915487119636964\n",
      "Epoch 162, rmse: 0.007359594572335482\n",
      "Epoch 162, Test Loss: 0.0018922669114544988\n",
      "Epoch 163, Loss: 0.006015655540977605\n",
      "Epoch 163, rmse: 0.010385889559984207\n",
      "Epoch 163, Test Loss: 0.002026855479925871\n",
      "Epoch 164, Loss: 0.00673480867408216\n",
      "Epoch 164, rmse: 0.008740703575313091\n",
      "Epoch 164, Test Loss: 0.0017587242182344198\n",
      "Epoch 165, Loss: 0.007297463373106439\n",
      "Epoch 165, rmse: 0.009678400121629238\n",
      "Epoch 165, Test Loss: 0.0019411921966820955\n",
      "Epoch 166, Loss: 0.010085956237162463\n",
      "Epoch 166, rmse: 0.008843418210744858\n",
      "Epoch 166, Test Loss: 0.001816494739614427\n",
      "Epoch 167, Loss: 0.005921553718508221\n",
      "Epoch 167, rmse: 0.006980393081903458\n",
      "Epoch 167, Test Loss: 0.0019052104325965047\n",
      "Epoch 168, Loss: 0.005612733104499057\n",
      "Epoch 168, rmse: 0.007112902123481035\n",
      "Epoch 168, Test Loss: 0.0018997545121237636\n",
      "Epoch 169, Loss: 0.005998961358272936\n",
      "Epoch 169, rmse: 0.0067648799158632755\n",
      "Epoch 169, Test Loss: 0.002027028240263462\n",
      "Epoch 170, Loss: 0.00723254552212893\n",
      "Epoch 170, rmse: 0.007407913915812969\n",
      "Epoch 170, Test Loss: 0.0018974501872435212\n",
      "Epoch 171, Loss: 0.005175429174414603\n",
      "Epoch 171, rmse: 0.006840623449534178\n",
      "Epoch 171, Test Loss: 0.0019195283530279994\n",
      "Epoch 172, Loss: 0.005696190131857293\n",
      "Epoch 172, rmse: 0.010072441771626472\n",
      "Epoch 172, Test Loss: 0.0019097531912848353\n",
      "Epoch 173, Loss: 0.007238047001010273\n",
      "Epoch 173, rmse: 0.006889943033456802\n",
      "Epoch 173, Test Loss: 0.001915834262035787\n",
      "Epoch 174, Loss: 0.00597909877251368\n",
      "Epoch 174, rmse: 0.0066706594079732895\n",
      "Epoch 174, Test Loss: 0.0019512154394760728\n",
      "Epoch 175, Loss: 0.005785098146589007\n",
      "Epoch 175, rmse: 0.009847170673310757\n",
      "Epoch 175, Test Loss: 0.001840027398429811\n",
      "Epoch 176, Loss: 0.006068637805583421\n",
      "Epoch 176, rmse: 0.007361673749983311\n",
      "Epoch 176, Test Loss: 0.001936890883371234\n",
      "Epoch 177, Loss: 0.005706069219741039\n",
      "Epoch 177, rmse: 0.007471572142094374\n",
      "Epoch 177, Test Loss: 0.0018965051276609302\n",
      "Epoch 178, Loss: 0.006277593096456258\n",
      "Epoch 178, rmse: 0.007976099848747253\n",
      "Epoch 178, Test Loss: 0.0017834085738286376\n",
      "Epoch 179, Loss: 0.007202933476946782\n",
      "Epoch 179, rmse: 0.007685686927288771\n",
      "Epoch 179, Test Loss: 0.0019434569403529167\n",
      "Epoch 180, Loss: 0.0053768707693961915\n",
      "Epoch 180, rmse: 0.006848476827144623\n",
      "Epoch 180, Test Loss: 0.0019342164741829038\n",
      "Epoch 181, Loss: 0.004911008774797665\n",
      "Epoch 181, rmse: 0.008365740068256855\n",
      "Epoch 181, Test Loss: 0.0018433652585372329\n",
      "Epoch 182, Loss: 0.0053695853202953\n",
      "Epoch 182, rmse: 0.0066412584856152534\n",
      "Epoch 182, Test Loss: 0.0019281955901533365\n",
      "Epoch 183, Loss: 0.005138328895554878\n",
      "Epoch 183, rmse: 0.006412053015083075\n",
      "Epoch 183, Test Loss: 0.0018566506914794445\n",
      "Epoch 184, Loss: 0.00489641293825116\n",
      "Epoch 184, rmse: 0.0070784431882202625\n",
      "Epoch 184, Test Loss: 0.0019034512806683779\n",
      "Epoch 185, Loss: 0.006240181628527353\n",
      "Epoch 185, rmse: 0.006737299729138613\n",
      "Epoch 185, Test Loss: 0.0018877139082178473\n",
      "Epoch 186, Loss: 0.005832070441101678\n",
      "Epoch 186, rmse: 0.0071873050183057785\n",
      "Epoch 186, Test Loss: 0.0019498764304444194\n",
      "Epoch 187, Loss: 0.004776040044816909\n",
      "Epoch 187, rmse: 0.00785001739859581\n",
      "Epoch 187, Test Loss: 0.0018992292461916804\n",
      "Epoch 188, Loss: 0.007124951829609927\n",
      "Epoch 188, rmse: 0.006656861398369074\n",
      "Epoch 188, Test Loss: 0.0019180176313966513\n",
      "Epoch 189, Loss: 0.004857296629779739\n",
      "Epoch 189, rmse: 0.006466452032327652\n",
      "Epoch 189, Test Loss: 0.0019344838801771402\n",
      "Epoch 190, Loss: 0.004966200540366117\n",
      "Epoch 190, rmse: 0.006627495400607586\n",
      "Epoch 190, Test Loss: 0.00188164331484586\n",
      "Epoch 191, Loss: 0.0043201072185183875\n",
      "Epoch 191, rmse: 0.006299238186329603\n",
      "Epoch 191, Test Loss: 0.0019216787768527865\n",
      "Epoch 192, Loss: 0.004784718712471658\n",
      "Epoch 192, rmse: 0.007445361465215683\n",
      "Epoch 192, Test Loss: 0.001915247761644423\n",
      "Epoch 193, Loss: 0.005296025366988033\n",
      "Epoch 193, rmse: 0.006443529389798641\n",
      "Epoch 193, Test Loss: 0.0019603087566792965\n",
      "Epoch 194, Loss: 0.004185144600342028\n",
      "Epoch 194, rmse: 0.006831927690654993\n",
      "Epoch 194, Test Loss: 0.0019441623007878661\n",
      "Epoch 195, Loss: 0.005158283220225712\n",
      "Epoch 195, rmse: 0.006521088071167469\n",
      "Epoch 195, Test Loss: 0.0018901010043919086\n",
      "Epoch 196, Loss: 0.005695906311302679\n",
      "Epoch 196, rmse: 0.006452460773289204\n",
      "Epoch 196, Test Loss: 0.002088255248963833\n",
      "Epoch 197, Loss: 0.00553769851103425\n",
      "Epoch 197, rmse: 0.006294126156717539\n",
      "Epoch 197, Test Loss: 0.0019040690967813134\n",
      "Epoch 198, Loss: 0.004579329932312248\n",
      "Epoch 198, rmse: 0.00584546709433198\n",
      "Epoch 198, Test Loss: 0.0019407478393986821\n",
      "Epoch 199, Loss: 0.005595945214736275\n",
      "Epoch 199, rmse: 0.006432189140468836\n",
      "Epoch 199, Test Loss: 0.0019612498581409454\n",
      "Epoch 200, Loss: 0.004223929539875826\n",
      "Epoch 200, rmse: 0.00602048309519887\n",
      "Epoch 200, Test Loss: 0.0019831403624266386\n",
      "Epoch 201, Loss: 0.004278622531273868\n",
      "Epoch 201, rmse: 0.006624608766287565\n",
      "Epoch 201, Test Loss: 0.0020117105450481176\n",
      "Epoch 202, Loss: 0.004262190137524158\n",
      "Epoch 202, rmse: 0.006412707269191742\n",
      "Epoch 202, Test Loss: 0.001944039948284626\n",
      "Epoch 203, Loss: 0.004430798224348109\n",
      "Epoch 203, rmse: 0.006049230229109526\n",
      "Epoch 203, Test Loss: 0.0018977244617417455\n",
      "Epoch 204, Loss: 0.004789581853401614\n",
      "Epoch 204, rmse: 0.006173017900437117\n",
      "Epoch 204, Test Loss: 0.0019279468106105924\n",
      "Epoch 205, Loss: 0.004722922287328402\n",
      "Epoch 205, rmse: 0.007446048315614462\n",
      "Epoch 205, Test Loss: 0.0020172635558992624\n",
      "Epoch 206, Loss: 0.005280554454657249\n",
      "Epoch 206, rmse: 0.006336988881230354\n",
      "Epoch 206, Test Loss: 0.0019082485232502222\n",
      "Epoch 207, Loss: 0.005396701290010242\n",
      "Epoch 207, rmse: 0.0059244344010949135\n",
      "Epoch 207, Test Loss: 0.0019923909567296505\n",
      "Epoch 208, Loss: 0.00400540742703015\n",
      "Epoch 208, rmse: 0.006683118175715208\n",
      "Epoch 208, Test Loss: 0.001961522037163377\n",
      "Epoch 209, Loss: 0.004483944707317278\n",
      "Epoch 209, rmse: 0.008216297253966331\n",
      "Epoch 209, Test Loss: 0.0019109136192128062\n",
      "Epoch 210, Loss: 0.004438303581991931\n",
      "Epoch 210, rmse: 0.006852682679891586\n",
      "Epoch 210, Test Loss: 0.0019852216355502605\n",
      "Epoch 211, Loss: 0.003916644458513474\n",
      "Epoch 211, rmse: 0.006083722226321697\n",
      "Epoch 211, Test Loss: 0.0019486744422465563\n",
      "Epoch 212, Loss: 0.004252310925949132\n",
      "Epoch 212, rmse: 0.006341326050460339\n",
      "Epoch 212, Test Loss: 0.0020309146493673325\n",
      "Epoch 213, Loss: 0.003745754002011381\n",
      "Epoch 213, rmse: 0.006163975689560175\n",
      "Epoch 213, Test Loss: 0.0019479884067550302\n",
      "Epoch 214, Loss: 0.00408633937331615\n",
      "Epoch 214, rmse: 0.00606076093390584\n",
      "Epoch 214, Test Loss: 0.0019388063810765743\n",
      "Epoch 215, Loss: 0.004655930446460843\n",
      "Epoch 215, rmse: 0.006152800749987364\n",
      "Epoch 215, Test Loss: 0.0019475636072456837\n",
      "Epoch 216, Loss: 0.004195625671854941\n",
      "Epoch 216, rmse: 0.006514004431664944\n",
      "Epoch 216, Test Loss: 0.0019630943424999714\n",
      "Epoch 217, Loss: 0.0042064851404575165\n",
      "Epoch 217, rmse: 0.006181363482028246\n",
      "Epoch 217, Test Loss: 0.0019153086468577385\n",
      "Epoch 218, Loss: 0.0036609584421967156\n",
      "Epoch 218, rmse: 0.006136864423751831\n",
      "Epoch 218, Test Loss: 0.0019477204186841846\n",
      "Epoch 219, Loss: 0.004125856356040458\n",
      "Epoch 219, rmse: 0.006167674902826548\n",
      "Epoch 219, Test Loss: 0.001947832410223782\n",
      "Epoch 220, Loss: 0.003972569847974228\n",
      "Epoch 220, rmse: 0.006372264586389065\n",
      "Epoch 220, Test Loss: 0.0019872088450938463\n",
      "Epoch 221, Loss: 0.003758565155294491\n",
      "Epoch 221, rmse: 0.006035711616277695\n",
      "Epoch 221, Test Loss: 0.001939774607308209\n",
      "Epoch 222, Loss: 0.003983614431490423\n",
      "Epoch 222, rmse: 0.00585668021813035\n",
      "Epoch 222, Test Loss: 0.0019851806573569775\n",
      "Epoch 223, Loss: 0.0036476758432399947\n",
      "Epoch 223, rmse: 0.006355504970997572\n",
      "Epoch 223, Test Loss: 0.0019448499660938978\n",
      "Epoch 224, Loss: 0.004051981659358717\n",
      "Epoch 224, rmse: 0.006091993302106857\n",
      "Epoch 224, Test Loss: 0.0019305242458358407\n",
      "Epoch 225, Loss: 0.003743161369129666\n",
      "Epoch 225, rmse: 0.0075578284449875355\n",
      "Epoch 225, Test Loss: 0.0019394619157537818\n",
      "Epoch 226, Loss: 0.004501625411649002\n",
      "Epoch 226, rmse: 0.0073831211775541306\n",
      "Epoch 226, Test Loss: 0.0019344487227499485\n",
      "Epoch 227, Loss: 0.003978531480242964\n",
      "Epoch 227, rmse: 0.006139648612588644\n",
      "Epoch 227, Test Loss: 0.0019416565774008632\n",
      "Epoch 228, Loss: 0.0036432812448765617\n",
      "Epoch 228, rmse: 0.006185281556099653\n",
      "Epoch 228, Test Loss: 0.0019506975077092648\n",
      "Epoch 229, Loss: 0.0037935850177746033\n",
      "Epoch 229, rmse: 0.006084815599024296\n",
      "Epoch 229, Test Loss: 0.001984743168577552\n",
      "Epoch 230, Loss: 0.004502336572841159\n",
      "Epoch 230, rmse: 0.007611210457980633\n",
      "Epoch 230, Test Loss: 0.0019695675000548363\n",
      "Epoch 231, Loss: 0.003552058999048313\n",
      "Epoch 231, rmse: 0.005850553046911955\n",
      "Epoch 231, Test Loss: 0.0018951159436255693\n",
      "Epoch 232, Loss: 0.003491769652100629\n",
      "Epoch 232, rmse: 0.006017456762492657\n",
      "Epoch 232, Test Loss: 0.001969653880223632\n",
      "Epoch 233, Loss: 0.003352617908603861\n",
      "Epoch 233, rmse: 0.00608281372115016\n",
      "Epoch 233, Test Loss: 0.0019861964974552393\n",
      "Epoch 234, Loss: 0.0034241063931403914\n",
      "Epoch 234, rmse: 0.007105117663741112\n",
      "Epoch 234, Test Loss: 0.0019731996580958366\n",
      "Epoch 235, Loss: 0.003455123838648433\n",
      "Epoch 235, rmse: 0.005877894349396229\n",
      "Epoch 235, Test Loss: 0.0019667537417262793\n",
      "Epoch 236, Loss: 0.0036998929590481566\n",
      "Epoch 236, rmse: 0.005913378670811653\n",
      "Epoch 236, Test Loss: 0.001956055173650384\n",
      "Epoch 237, Loss: 0.003659369440356386\n",
      "Epoch 237, rmse: 0.0058958749286830425\n",
      "Epoch 237, Test Loss: 0.0019835461862385273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 290\u001b[0m\n\u001b[0;32m    285\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# 存储当前 epoch 的预测值\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (data ,label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    287\u001b[0m         \n\u001b[0;32m    288\u001b[0m             \u001b[38;5;66;03m# data1 = data.squeeze(1).cuda() # 删除data张量中的第一个维度，并将其移动到GPU\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# print(f'data shape before unsqueeze: {data.shape}')\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m     pred\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 将data1封装成Variable并传入模型进行前向传播，得到预测值\u001b[39;00m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;66;03m# print(f'pred shape : {pred.shape}')\u001b[39;00m\n\u001b[0;32m    292\u001b[0m             \u001b[38;5;66;03m# pred = pred[1, :, :]  # 这里取pred的第二个维度的数据作为最终预测结果\u001b[39;00m\n\u001b[0;32m    293\u001b[0m             \u001b[38;5;66;03m# print(pred.shape)\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()  \u001b[38;5;66;03m# 将标签数据添加一个维度并移动到GPU\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:118\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\layers\\layer.py:830\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    828\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 830\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[0;32m    834\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\backend\\torch\\layer.py:26\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Operation\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:118\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\ops\\operation.py:42\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m     38\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[0;32m     39\u001b[0m         call_fn,\n\u001b[0;32m     40\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     41\u001b[0m     )\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:157\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\models\\functional.py:188\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m             x\u001b[38;5;241m.\u001b[39m_keras_mask \u001b[38;5;241m=\u001b[39m mask\n\u001b[1;32m--> 188\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_through_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\ops\\function.py:153\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[1;34m(self, inputs, operation_fn)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mfill_in(tensor_dict)\n\u001b[1;32m--> 153\u001b[0m outputs \u001b[38;5;241m=\u001b[39m operation_fn(node\u001b[38;5;241m.\u001b[39moperation)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39moutputs, tree\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\models\\functional.py:572\u001b[0m, in \u001b[0;36moperation_fn.<locals>.call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(operation, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_has_training_arg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m operation\u001b[38;5;241m.\u001b[39m_call_has_training_arg\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    570\u001b[0m ):\n\u001b[0;32m    571\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m training\n\u001b[1;32m--> 572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m operation(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:118\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\layers\\layer.py:830\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    828\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 830\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[0;32m    834\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\backend\\torch\\layer.py:26\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Operation\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:118\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\ops\\operation.py:42\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m     38\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[0;32m     39\u001b[0m         call_fn,\n\u001b[0;32m     40\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     41\u001b[0m     )\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:157\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\layers\\rnn\\gru.py:559\u001b[0m, in \u001b[0;36mGRU.call\u001b[1;34m(self, sequences, initial_state, mask, training)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, sequences, initial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_state\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:397\u001b[0m, in \u001b[0;36mRNN.call\u001b[1;34m(self, sequences, initial_state, mask, training)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# Prepopulate the dropout state so that the inner_loop is stateless\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# this is particularly important for JAX backend.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_config_dropout_masks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell, sequences, initial_state)\n\u001b[1;32m--> 397\u001b[0m last_output, outputs, states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m last_output \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(last_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m    404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\layers\\rnn\\gru.py:554\u001b[0m, in \u001b[0;36mGRU.inner_loop\u001b[1;34m(self, sequences, initial_state, mask, training)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:339\u001b[0m, in \u001b[0;36mRNN.inner_loop\u001b[1;34m(self, sequences, initial_state, mask, training)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mis_nested(initial_state):\n\u001b[0;32m    337\u001b[0m     initial_state \u001b[38;5;241m=\u001b[39m [initial_state]\n\u001b[1;32m--> 339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgo_backwards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgo_backwards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munroll\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzero_output_for_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_output_for_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_all_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\backend\\torch\\rnn.py:344\u001b[0m, in \u001b[0;36mrnn\u001b[1;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask, return_all_outputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m new_states \u001b[38;5;241m=\u001b[39m states\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m time \u001b[38;5;241m<\u001b[39m time_steps_t \u001b[38;5;129;01mand\u001b[39;00m it \u001b[38;5;241m<\u001b[39m max_iterations:\n\u001b[1;32m--> 344\u001b[0m     final_outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_ta_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     time, output_ta_t \u001b[38;5;241m=\u001b[39m final_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    346\u001b[0m     new_states \u001b[38;5;241m=\u001b[39m final_outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\backend\\torch\\rnn.py:327\u001b[0m, in \u001b[0;36mrnn.<locals>._step\u001b[1;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[0;32m    325\u001b[0m current_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ta[time] \u001b[38;5;28;01mfor\u001b[39;00m ta \u001b[38;5;129;01min\u001b[39;00m input_ta)\n\u001b[0;32m    326\u001b[0m current_input \u001b[38;5;241m=\u001b[39m pack_sequence_as(inputs, current_input)\n\u001b[1;32m--> 327\u001b[0m output, new_states \u001b[38;5;241m=\u001b[39m \u001b[43mstep_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m flat_new_state \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(new_states)\n\u001b[0;32m    332\u001b[0m flat_output \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(output)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:331\u001b[0m, in \u001b[0;36mRNN.inner_loop.<locals>.step\u001b[1;34m(inputs, states)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(inputs, states):\n\u001b[1;32m--> 331\u001b[0m     output, new_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell(inputs, states, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcell_kwargs)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mis_nested(new_states):\n\u001b[0;32m    333\u001b[0m         new_states \u001b[38;5;241m=\u001b[39m [new_states]\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:118\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\layers\\layer.py:830\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    828\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 830\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[0;32m    834\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\backend\\torch\\layer.py:26\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Operation\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:118\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\ops\\operation.py:42\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m     38\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[0;32m     39\u001b[0m         call_fn,\n\u001b[0;32m     40\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     41\u001b[0m     )\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:157\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\layers\\rnn\\gru.py:273\u001b[0m, in \u001b[0;36mGRUCell.call\u001b[1;34m(self, inputs, states, training)\u001b[0m\n\u001b[0;32m    270\u001b[0m recurrent_r \u001b[38;5;241m=\u001b[39m matrix_inner[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    271\u001b[0m recurrent_h \u001b[38;5;241m=\u001b[39m matrix_inner[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[1;32m--> 273\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_z\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecurrent_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_activation(x_r \u001b[38;5;241m+\u001b[39m recurrent_r)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_after:\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\activations\\activations.py:334\u001b[0m, in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.activations.sigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(x):\n\u001b[0;32m    319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sigmoid activation function.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    It is defined as: `sigmoid(x) = 1 / (1 + exp(-x))`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m        x: Input tensor.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;66;03m# Cache the logits to use for crossentropy loss.\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\ops\\nn.py:107\u001b[0m, in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sigmoid()\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\keras\\src\\backend\\torch\\nn.py:32\u001b[0m, in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(x):\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m convert_to_tensor(x)\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\nn\\functional.py:2013\u001b[0m, in \u001b[0;36msigmoid\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   2006\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(\u001b[38;5;28minput\u001b[39m):  \u001b[38;5;66;03m# noqa: D400,D402\u001b[39;00m\n\u001b[0;32m   2007\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"sigmoid(input) -> Tensor\u001b[39;00m\n\u001b[0;32m   2008\u001b[0m \n\u001b[0;32m   2009\u001b[0m \u001b[38;5;124;03m    Applies the element-wise function :math:`\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}`\u001b[39;00m\n\u001b[0;32m   2010\u001b[0m \n\u001b[0;32m   2011\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.nn.Sigmoid` for more details.\u001b[39;00m\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Reshape,Permute,BatchNormalization,SimpleRNN,GlobalAveragePooling1D\n",
    "from keras.layers import Input, Concatenate,Dropout,Layer,Bidirectional,GRU,Activation, Dropout, Add, Dense, Lambda,LayerNormalization,MultiHeadAttention\n",
    "from torch.optim.lr_scheduler import OneCycleLR,StepLR\n",
    "from torch.optim import adamw\n",
    "from torch.optim import SGD\n",
    "import torch\n",
    "from torch_optimizer import Lamb\n",
    "from keras.regularizers import l2,l1\n",
    "# import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tcn import TCN  # 从keras-tcn库中导入TCN层\n",
    "from matplotlib import font_manager\n",
    "import tensorflow as tf\n",
    "from keras_efficient_kan import KANLinear\n",
    "from tcn import TCN \n",
    "from keras_transformer import get_model\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "# import keras_nlp\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "font_path = 'C:/Windows/Fonts/simsun.ttc'  # 示例：使用微软雅黑\n",
    "font_prop = font_manager.FontProperties(fname=font_path)\n",
    "\n",
    "# transformer CNNtransformer\n",
    "# # 参数\n",
    "# sequence_length = 200\n",
    "# feature_dim = 3\n",
    "# embed_dim = 64\n",
    "# num_heads = 4\n",
    "# ff_dim = 128\n",
    "# num_layers = 2\n",
    "\n",
    "\n",
    "# class PositionalEncoding(tf.keras.layers.Layer):\n",
    "#     def __init__(self, sequence_length, embed_dim):\n",
    "#         super().__init__()\n",
    "#         self.position_embed = keras.layers.Embedding(\n",
    "#             input_dim=sequence_length, \n",
    "#             output_dim=embed_dim\n",
    "#         )\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "#         positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "#         return inputs + self.position_embed(positions)\n",
    "\n",
    "# def transformer_encoder(x, num_heads, embed_dim, ff_dim):\n",
    "#     # 多头注意力\n",
    "#     attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "#     \n",
    "#     x = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "    \n",
    "#     # FeedForward\n",
    "#     ff_output = Dense(ff_dim, activation=\"relu\")(x)\n",
    "#     ff_output = Dense(embed_dim)(ff_output)\n",
    "#    \n",
    "#     x = LayerNormalization(epsilon=1e-6)(x + ff_output)\n",
    "#     return x\n",
    "\n",
    "# model_input = Input(shape=(200,3))\n",
    "\n",
    "# x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(model_input)  # (None, 5, 32)\n",
    "# # 特征升维 + 位置编码\n",
    "# x = Dense(embed_dim)(x)\n",
    "# x = PositionalEncoding(sequence_length, embed_dim)(x)\n",
    "\n",
    "# # 堆叠Transformer编码器\n",
    "# for _ in range(num_layers):\n",
    "#     x = transformer_encoder(x, num_heads, embed_dim, ff_dim)\n",
    "\n",
    "# # 取最后一个时间步 [batch_size, embed_dim]\n",
    "# last_timestep = x[:, -1, :]\n",
    "# output = Dense(units=1, activation='linear')(last_timestep)\n",
    "\n",
    "\n",
    "# KANGRID4\n",
    "# model_input = Input(shape=(3,))\n",
    "# # # x = Dense(units=100, activation='linear')(model_input)\n",
    "# x = KANLinear(100, use_layernorm=False)(model_input)\n",
    "# x = KANLinear(100, use_layernorm=False)(x)\n",
    "# x = KANLinear(100, use_layernorm=False)(x)\n",
    "# x = KANLinear(100, use_layernorm=False)(x)\n",
    "# output = Dense(units=1, activation='linear')(x)\n",
    "\n",
    "# MLP\n",
    "# x = Dense(units=70, activation='relu')(model_input)\n",
    "# x = Dense(units=70, activation='relu')(x)\n",
    "# x = Dense(units=70, activation='relu')(x)\n",
    "# x = Dense(units=70, activation='relu')(x)\n",
    "# output = Dense(units=1, activation='linear')(x)\n",
    "\n",
    "# TKAN,LSTM,GRU,TCN,CNNGRU,CNNLSTM\n",
    "model_input = Input(shape=(200,3))\n",
    "\n",
    "# # TCN 层\n",
    "# x = TCN(nb_filters=256,kernel_size=7, dilations=[1,2,4,8])(model_input)\n",
    "\n",
    "# output = Dense(units=1, activation='linear')(x)\n",
    "\n",
    "# 添加卷积层\n",
    "# x = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(model_input)  # (None, 10, 16)\n",
    "# x = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(x)  # (None, 5, 32)\n",
    "# x = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(x)  # (None, 5, 32)\n",
    "\n",
    "\n",
    "x= TKAN(100,sub_kan_output_dim = 200, sub_kan_input_dim = 100, return_sequences=True)(model_input)\n",
    "lstm_output2= TKAN(100,sub_kan_output_dim = 200, sub_kan_input_dim = 100, return_sequences=False)(x)\n",
    "\n",
    "# x=LSTM(60,return_sequences=True)(model_input)\n",
    "# lstm_output2=LSTM(60,return_sequences=False)(x)\n",
    "\n",
    "# x=GRU(100,return_sequences=True)(model_input)\n",
    "# lstm_output2=GRU(100,return_sequences=False)(x)\n",
    "# # 最后输出层\n",
    "output = Dense(units=1, activation='linear')(lstm_output2)\n",
    "\n",
    "# 创建模型\n",
    "model = Model(inputs=model_input, outputs=output)\n",
    "model.to(device)\n",
    "\n",
    "lr_values = []\n",
    "criterion = nn.MSELoss()  # 定义损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam梯度下降  学习率=0.001\n",
    "\n",
    "total_params = model.count_params()\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "\n",
    "train_files = ['E:/python/TKAN-main(1)/TKAN-main/data/20C/DST_20.csv', 'E:/python/TKAN-main(1)/TKAN-main/data/20C/FUDS_20.csv','E:/python/TKAN-main(1)/TKAN-main/data/25C/DST_25.csv', 'E:/python/TKAN-main(1)/TKAN-main/data/25C/FUDS_25.csv',\n",
    "               'E:/python/TKAN-main(1)/TKAN-main/data/30C/DST_30.csv','E:/python/TKAN-main(1)/TKAN-main/data/30C/FUDS_30.csv','E:/python/TKAN-main(1)/TKAN-main/data/40C/DST_40.csv','E:/python/TKAN-main(1)/TKAN-main/data/0C/DST_0.csv'\n",
    "                   ,'E:/python/TKAN-main(1)/TKAN-main/data/40C/FUDS_40.csv','E:/python/TKAN-main(1)/TKAN-main/data/50C/DST_50.csv','E:/python/TKAN-main(1)/TKAN-main/data/50C/FUDS_50.csv','E:/python/TKAN-main(1)/TKAN-main/data/0C/FUDS_0.csv'\n",
    "                   ,'E:/python/TKAN-main(1)/TKAN-main/data/10C/DST_10.csv', 'E:/python/TKAN-main(1)/TKAN-main/data/10C/FUDS_10.csv']\n",
    "test_file = ['E:/python/TKAN-main(1)/TKAN-main/data/20C/US06_20.csv','E:/python/TKAN-main(1)/TKAN-main/data/0C/US06_0.csv','E:/python/TKAN-main(1)/TKAN-main/data/30C/US06_30.csv','E:/python/TKAN-main(1)/TKAN-main/data/25C/US06_25.csv',\n",
    "             'E:/python/TKAN-main(1)/TKAN-main/data/40C/US06_40.csv','E:/python/TKAN-main(1)/TKAN-main/data/50C/US06_50.csv','E:/python/TKAN-main(1)/TKAN-main/data/10C/US06_10.csv']\n",
    "\n",
    "batch_size =1024\n",
    "# train_loader, test_loader , close_max, close_min = getData_ANN(train_files, test_file, batch_size,batch_size1=8192,shuf=True)KAN,MLP\n",
    "train_loader, test_loader  = getData(train_files, test_file, batch_size,batch_size1=8192,shuf=True,sequence=200)\n",
    "\n",
    "scheduler = StepLR(optimizer,step_size=10,gamma=0.9)\n",
    "\n",
    "epoch_losses = []\n",
    "labels = []  # 用于保存所有的真实值\n",
    "preds = []  # 用于保存所有的预测值\n",
    "best_rmse = float('inf')  # 初始化最佳RMSE为无穷大\n",
    "best_r2 = -float('inf')  # 初始化最佳R^2为负无穷大\n",
    "best_loss = float('inf')  # 初始化最佳R^2为负无穷大\n",
    "batch_train_losses = []  # 记录每个batch的测试损失\n",
    "train_rmse_list = []  # 记录每个epoch的训练集RMSE\n",
    "test_rmse_list = []  # 记录每个epoch的测试集RMSE\n",
    "test_epoch_losses = []\n",
    "\n",
    "with open('0.2CNNTKAN_loss_log1no验证grid3,无限.txt', 'w') as f,open('0.2Gtest_loss_logCNNTKAN1no验证grid3,无限.txt', 'w') as f_rmse:\n",
    "    for i in range(400):  # 循环遍历每个epoch\n",
    "        test_loss = 0\n",
    "        valid_loss = 0\n",
    "        train_rmse = 0  # 初始化当前epoch的RMSE\n",
    "        total_loss = 0  # 初始化当前epoch的总损失\n",
    "        all_labels = []  # 存储当前 epoch 的真实值\n",
    "        all_preds = []  # 存储当前 epoch 的预测值\n",
    "        for idx, (data ,label) in enumerate(train_loader):\n",
    "        \n",
    "            pred= model((data).cuda()) # 将data1封装成Variable并传入模型进行前向传播，得到预测值\n",
    "            \n",
    "            label = label.unsqueeze(1).cuda()  # 将标签数据添加一个维度并移动到GPU\n",
    "                    \n",
    "            loss = criterion(pred, label) # 计算当前batch的损失值\n",
    "            optimizer.zero_grad() # 清空优化器的梯度\n",
    "           \n",
    "            loss.backward()   # 反向传播，计算梯度\n",
    "            optimizer.step() # 更新模型参数\n",
    "           \n",
    "            total_loss += loss.item()   # 累加当前batch的损失值到total_loss\n",
    "            rmse = torch.sqrt(nn.MSELoss()(pred, label))  # 均方根误差\n",
    "            batch_train_losses.append(loss.item())  # 记录每个batch的损失\n",
    "\n",
    "\n",
    "                # 保存预测值和真实值\n",
    "            all_preds.append(pred.detach().cpu().numpy())\n",
    "            all_labels.append(label.detach().cpu().numpy())\n",
    "            train_rmse += rmse.item()  # 累加当前batch的RMSE\n",
    "\n",
    "        train_rmse /= len(train_loader)  # 计算平均RMSE\n",
    "        train_rmse_list.append(train_rmse)  # 添加到训练集RMSE列表\n",
    "        scheduler.step()        # 获取当前学习率并添加到列表中\n",
    "        \n",
    "\n",
    "                # 转换为numpy数组并合并数据\n",
    "        labels = np.concatenate(all_labels)\n",
    "        preds = np.concatenate(all_preds)\n",
    "            # 记录每个epoch的总损失\n",
    "        epoch_losses.append(total_loss)\n",
    "            # 在终端输出第多少轮和对应的loss\n",
    "        print(f'Epoch {i+1}, Loss: {total_loss}')\n",
    "        print(f'Epoch {i + 1}, rmse: {rmse}')\n",
    "        # print(f'Epoch {i + 1}, R2: {R2}')\n",
    "            \n",
    "            # 将损失写入文件\n",
    "        f.write(f'Epoch {i+1}, Loss: {total_loss}\\n')\n",
    "       \n",
    "        model.eval()  # 将模型设置为评估模式\n",
    "        with torch.no_grad():  # 关闭梯度计算\n",
    "            test_preds = []\n",
    "            test_labels = []\n",
    "            for data, label in test_loader:\n",
    "                data, label = data.cuda(), label.cuda()\n",
    "                label = label.unsqueeze(1).cuda()\n",
    "                pred = model(data)\n",
    "                test_preds.append(pred.cpu().numpy())\n",
    "                test_labels.append(label.cpu().numpy())\n",
    "                # 计算当前批次的损失\n",
    "                batch_loss = criterion(pred, label)\n",
    "                test_loss += batch_loss.item()\n",
    "            \n",
    "            test_preds = np.concatenate(test_preds)\n",
    "            test_labels = np.concatenate(test_labels)\n",
    "            \n",
    "            # 计算测试集上的RMSE和R^2\n",
    "            test_loss = nn.MSELoss()(torch.tensor(test_preds), torch.tensor(test_labels))\n",
    "           \n",
    "            print(f'Epoch {i + 1}, Test Loss: {test_loss}')\n",
    "            \n",
    "            f_rmse.write(f'Epoch {i+1}, Test LOSS: {test_loss}\\n')\n",
    "          \n",
    "            # 保存最佳模型\n",
    "            if test_loss<best_loss :\n",
    "                best_loss = min(best_loss, test_loss)\n",
    "                # best_r2 = max(best_r2, test_r2)\n",
    "                torch.save({'state_dict': model.state_dict()},'model/2GRU300.200.100UNIT.XIAJIANG10.0.9.pth')\n",
    "                print('New best model saved.')\n",
    "        \n",
    "        model.train()  # 将模型设置回训练模式\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57165471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All predictions shape: (47602, 1), All labels shape: (47602, 1)\n",
      "预测值和标签已保存到 ZZ.xlsx\n",
      "Labels shape: (47602, 1), Predictions shape: (47602, 1)\n",
      "LOSS: 0.000157\n",
      "RMSE: 0.012522\n",
      "MAE: 0.009215\n",
      "r^2: 0.997988\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import font_manager\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Reshape,Dropout\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras_efficient_kan import KANLinear\n",
    "from tcn import TCN \n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Concatenate,Dropout,Layer,Bidirectional,GRU,Activation, Dropout, Add, Dense, Lambda,LayerNormalization,MultiHeadAttention\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "font_path = 'C:/Windows/Fonts/simsun.ttc'  # 示例：使用微软雅黑\n",
    "font_prop = font_manager.FontProperties(fname=font_path)\n",
    "\n",
    "# transformer CNNtransformer\n",
    "# # 参数\n",
    "# sequence_length = 200\n",
    "# feature_dim = 3\n",
    "# embed_dim = 64\n",
    "# num_heads = 4\n",
    "# ff_dim = 128\n",
    "# num_layers = 2\n",
    "\n",
    "\n",
    "# class PositionalEncoding(tf.keras.layers.Layer):\n",
    "#     def __init__(self, sequence_length, embed_dim):\n",
    "#         super().__init__()\n",
    "#         self.position_embed = keras.layers.Embedding(\n",
    "#             input_dim=sequence_length, \n",
    "#             output_dim=embed_dim\n",
    "#         )\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "#         positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "#         return inputs + self.position_embed(positions)\n",
    "\n",
    "# def transformer_encoder(x, num_heads, embed_dim, ff_dim):\n",
    "#     # 多头注意力\n",
    "#     attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "#     \n",
    "#     x = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "    \n",
    "#     # FeedForward\n",
    "#     ff_output = Dense(ff_dim, activation=\"relu\")(x)\n",
    "#     ff_output = Dense(embed_dim)(ff_output)\n",
    "#    \n",
    "#     x = LayerNormalization(epsilon=1e-6)(x + ff_output)\n",
    "#     return x\n",
    "\n",
    "# model_input = Input(shape=(200,3))\n",
    "\n",
    "# x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(model_input)  # (None, 5, 32)\n",
    "# # 特征升维 + 位置编码\n",
    "# x = Dense(embed_dim)(x)\n",
    "# x = PositionalEncoding(sequence_length, embed_dim)(x)\n",
    "\n",
    "# # 堆叠Transformer编码器\n",
    "# for _ in range(num_layers):\n",
    "#     x = transformer_encoder(x, num_heads, embed_dim, ff_dim)\n",
    "\n",
    "# # 取最后一个时间步 [batch_size, embed_dim]\n",
    "# last_timestep = x[:, -1, :]\n",
    "# output = Dense(units=1, activation='linear')(last_timestep)\n",
    "\n",
    "\n",
    "# KANGRID4\n",
    "# model_input = Input(shape=(3,))\n",
    "# # # x = Dense(units=100, activation='linear')(model_input)\n",
    "# x = KANLinear(100, use_layernorm=False)(model_input)\n",
    "# x = KANLinear(100, use_layernorm=False)(x)\n",
    "# x = KANLinear(100, use_layernorm=False)(x)\n",
    "# x = KANLinear(100, use_layernorm=False)(x)\n",
    "# output = Dense(units=1, activation='linear')(x)\n",
    "\n",
    "# MLP\n",
    "# x = Dense(units=70, activation='relu')(model_input)\n",
    "# x = Dense(units=70, activation='relu')(x)\n",
    "# x = Dense(units=70, activation='relu')(x)\n",
    "# x = Dense(units=70, activation='relu')(x)\n",
    "# output = Dense(units=1, activation='linear')(x)\n",
    "\n",
    "# TKAN,LSTM,GRU,TCN,CNNGRU,CNNLSTM\n",
    "model_input = Input(shape=(200,3))\n",
    "\n",
    "# # TCN 层\n",
    "# x = TCN(nb_filters=256,kernel_size=7, dilations=[1,2,4,8])(model_input)\n",
    "\n",
    "# output = Dense(units=1, activation='linear')(x)\n",
    "\n",
    "# 添加卷积层\n",
    "# x = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(model_input)  # (None, 10, 16)\n",
    "# x = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(x)  # (None, 5, 32)\n",
    "# x = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')(x)  # (None, 5, 32)\n",
    "\n",
    "\n",
    "x= TKAN(100,sub_kan_output_dim = 200, sub_kan_input_dim = 100, return_sequences=True)(model_input)\n",
    "lstm_output2= TKAN(100,sub_kan_output_dim = 200, sub_kan_input_dim = 100, return_sequences=False)(x)\n",
    "\n",
    "# x=LSTM(60,return_sequences=True)(model_input)\n",
    "# lstm_output2=LSTM(60,return_sequences=False)(x)\n",
    "\n",
    "# x=GRU(100,return_sequences=True)(model_input)\n",
    "# lstm_output2=GRU(100,return_sequences=False)(x)\n",
    "# # 最后输出层\n",
    "output = Dense(units=1, activation='linear')(lstm_output2)\n",
    "\n",
    "# # 创建模型\n",
    "model = Model(inputs=model_input, outputs=output)\n",
    "model.to(device)  # 将模型移到指定的设备（CPU或GPU）\n",
    "checkpoint = torch.load('model/2TKAN300.B0.69.GRID1.100UNIT.XIAJIANG10.0.9.pth', weights_only=True)  # 加载模型的状态字典\n",
    "model.load_state_dict(checkpoint['state_dict']) # 将状态字典加载到模型中\n",
    "preds = [] # 初始化预测值列表\n",
    "labels = []  # 初始化真实标签值列表\n",
    "total_loss = 0\n",
    "criterion = nn.MSELoss()\n",
    "    \n",
    "train_files = ['E:/python/TKAN-main(1)/TKAN-main/data/20C/DST_20.csv', 'E:/python/TKAN-main(1)/TKAN-main/data/20C/FUDS_20.csv','E:/python/TKAN-main(1)/TKAN-main/data/25C/DST_25.csv', 'E:/python/TKAN-main(1)/TKAN-main/data/25C/FUDS_25.csv',\n",
    "               'E:/python/TKAN-main(1)/TKAN-main/data/30C/DST_30.csv','E:/python/TKAN-main(1)/TKAN-main/data/30C/FUDS_30.csv','E:/python/TKAN-main(1)/TKAN-main/data/40C/DST_40.csv','E:/python/TKAN-main(1)/TKAN-main/data/0C/DST_0.csv'\n",
    "                   ,'E:/python/TKAN-main(1)/TKAN-main/data/40C/FUDS_40.csv','E:/python/TKAN-main(1)/TKAN-main/data/50C/DST_50.csv','E:/python/TKAN-main(1)/TKAN-main/data/50C/FUDS_50.csv','E:/python/TKAN-main(1)/TKAN-main/data/0C/FUDS_0.csv'\n",
    "                   ,'E:/python/TKAN-main(1)/TKAN-main/data/10C/DST_10.csv', 'E:/python/TKAN-main(1)/TKAN-main/data/10C/FUDS_10.csv']\n",
    "\n",
    "test_file = ['E:/python/TKAN-main(1)/TKAN-main/data/20C/US06_20.csv','E:/python/TKAN-main(1)/TKAN-main/data/0C/US06_0.csv','E:/python/TKAN-main(1)/TKAN-main/data/30C/US06_30.csv','E:/python/TKAN-main(1)/TKAN-main/data/25C/US06_25.csv',\n",
    "             'E:/python/TKAN-main(1)/TKAN-main/data/40C/US06_40.csv','E:/python/TKAN-main(1)/TKAN-main/data/50C/US06_50.csv','E:/python/TKAN-main(1)/TKAN-main/data/10C/US06_10.csv']\n",
    "\n",
    "batch_size = 2\n",
    "# train_loader, test_loader , close_max, close_min = getData_ANN(train_files, test_file, batch_size,batch_size1=8192,shuf=False)KAN,MLP\n",
    "train_loader, test_loader = getData(train_files, test_file, batch_size,batch_size1=4096,shuf=False,sequence=200)\n",
    "\n",
    "# 初始化存储预测和标签的列表\n",
    "preds = []\n",
    "labels = []\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "with torch.no_grad():  # 关闭梯度计算\n",
    "    for idx, (data, label) in enumerate(test_loader):   # 遍历测试数据进行预测\n",
    "        data = data.cuda()  # Move data to the specified device\n",
    "        label = label.unsqueeze(1).cuda() # Add dimension and move to device\n",
    "        pred = model(data)\n",
    "        preds.append(pred.detach().cpu().numpy())\n",
    "        labels.append(label.detach().cpu().numpy())\n",
    "\n",
    "# data=pd.read_csv(r\"data/10C/DST_10C.csv\")\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "print(f\"All predictions shape: {preds.shape}, All labels shape: {labels.shape}\")\n",
    "\n",
    "    # 计算评价指标\n",
    "mse = nn.MSELoss()(torch.tensor(preds), torch.tensor(labels)) # 均方误差\n",
    "rmse = torch.sqrt(nn.MSELoss()(torch.tensor(preds), torch.tensor(labels)))  # 均方根误差\n",
    "mae = torch.mean(torch.abs(torch.tensor(preds) - torch.tensor(labels)))  # 平均绝对误差\n",
    "\n",
    "r2=r2_score(labels, preds)\n",
    "\n",
    "\n",
    "labels = np.array(labels)\n",
    "preds = np.array(preds)\n",
    "df = pd.DataFrame({\n",
    "    'True Labels': labels.flatten(),\n",
    "    'Predictions': preds.flatten()\n",
    "})\n",
    "\n",
    "# 定义保存 Excel 文件的路径\n",
    "excel_filename = 'ZZ.xlsx'\n",
    "\n",
    "# 保存 DataFrame 到 Excel\n",
    "df.to_excel(excel_filename, index=False)\n",
    "\n",
    "print(f\"预测值和标签已保存到 {excel_filename}\")\n",
    "\n",
    "print(f\"Labels shape: {labels.shape}, Predictions shape: {preds.shape}\")\n",
    "print(f'LOSS: {mse.item():.6f}')\n",
    "print(f'RMSE: {rmse.item():.6f}')\n",
    "print(f'MAE: {mae.item():.6f}')\n",
    "print(f'r^2: {r2:.6f}')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(labels, label='真实值', color='blue', linestyle='-')\n",
    "plt.plot(preds, label='预测值', color='orange', linestyle='--')\n",
    "plt.title('真实值与预测值随Epoch变化的直线图',fontproperties=font_prop)\n",
    "plt.xlabel('NUM')\n",
    "plt.ylabel('值',fontproperties=font_prop)\n",
    "plt.legend(prop=font_prop)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
